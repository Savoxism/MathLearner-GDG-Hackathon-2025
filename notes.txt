Dataset used: MetaMaths (2021)
Model used: GPT-4o Mini / Gemini 2.5 Flash Thinking

The Learning Module
Input: Orginal problem + Original solution
Output: Modified solution (standardised solution + Python code + problem features)
Receiving Examples: The process commences with the LLM being exposed to a diverse set of mathematical problems and their detailed solutions. Datasets such as MATH, which contain challenging problems with step-by-step solutions, are typically used for this purpose.   

Modified Solutions Generation and Verification: Based on the provided examples, the LLM generates modified solutions, often structured as executable programs (e.g., using a method like Parsel). A crucial aspect of this stage is verification: these generated programs are executed to confirm their correctness. If a solution fails this verification step (e.g., produces an incorrect answer or an error), the LLM is prompted to generate a revised version. This iterative refinement continues until a correct, verifiable solution is obtained.

Feature Generation and Storage: For each problem and its verified solution, the LLM then generates two categories of features:
+ A general description of the problem type (e.g., algebra, geometry, calculus).

+ Features that describe the specific operations, theorems, or techniques employed in each step of the solution. These textual features are subsequently translated into numerical vector representations (embeddings) and stored in a vector database. This database serves as MathLearner's "memory," enabling efficient similarity searches for future problem-solving efforts.

The Application Module
Input: New problem 
Output: features of new problem
Receiving Questions: The LLM receives a new mathematical problem as input.

Extracting Features from Problems: Analogous to the feature generation process in the Learning Module, the LLM extracts both general and step-specific features from this new problem.

Feature Matching and Answer Generation: The extracted features from the new problem are converted into vector embeddings. These embeddings are then used to perform a similarity search within the vector database populated by the Learning Module. The outcome of this search dictates the subsequent steps:
+ Successful Match: If the search identifies previously stored solutions or solution methods with similar feature vectors, these retrieved "ideas" (e.g., solution steps, relevant theorems) are combined with the original new problem statement. This augmented information is then provided as input to the LLM, which uses it to generate a solution for the new problem.

+ No Match: If the similarity search does not yield any sufficiently similar features or solution methods in the database, the new problem is passed directly to the LLM. In this scenario, the LLM attempts to generate a solution based solely on its pre-trained knowledge, without the benefit of retrieved information from MathLearner's specialized memory.

Generation Stage
Input: New problem + other SUCCESSFULLY RETRIEVED problems / New problem
Output: Python code for new problem

The verification step is very important: if a generated program yields an incorrect result or fails to execute, the system can identify the flaw and prompt the LLM to revise its solution. This iterative process of generation and verification encourages the LLM to produce more structured and logically sound reasoning pathways that are computationally verifiable.

The Inference Pipeline
1) Feature extraction: LLM will read the problem and extract features
2) Features are embedded into vector representations, then they will be used to find k nearest neighbors iniside vectorDB to obtain similar solutions
3) New input is fed into the LLM and it is made to generate executable Python code + additional documentation
4) Execute the Python code externally (via a Python interpreter). If arrvied at a solution, it must be verified. If otherwise, the LLM is forced to generate another Python program until it becomes verifiable.

METRICS
+ Global Accuracy: The overall proportion of problems correctly solved out of all problems attempted. 

+ Profitability (Benefit): This metric was defined to quantify the extent to which successfully finding and utilizing similar results (retrieved solutions/methods) contributes to generating correct solutions for new problems. It isolates the benefit derived specifically when the retrieval component functions as intended and leads to a correct outcome.   

+ Precision Accuracy: This metric assesses the accuracy of the system specifically in scenarios where the new questions successfully find a similar solution idea in the knowledge base. 

+ Target Achievement Rate: This metric measures the framework’s effectiveness in enabling the LLM to use retrieved solutions to correctly answer problems that the CoT baseline failed to address. It signifies that the framework is not merely improving accuracy on problems that CoT could already handle (albeit perhaps less consistently) but is genuinely expanding the set of solvable problems for the LLM. By providing crucial scaffolding in the form of retrieved solution strategies, MathLearner enables the LLM to tackle challenges that would otherwise be beyond its unassisted CoT capabilities.   
